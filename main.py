import os
import csv
from file_utils import get_files_in_folder, read_text_file, write_csv_file, read_csv_file
from text_utils import count_words, count_unique_words, count_lines, get_most_common_words, calculate_ttr

def analyze_text_file(filepath):
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ–¥–∏–Ω —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª –∏ –≤—ã–≤–æ–¥–∏—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É.

    Args:
        filepath (str): –ü—É—Ç—å –∫ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É —Ñ–∞–π–ª—É 
    """
    print("=" * 70)
    print(f"üìä –ê–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–∞: {os.path.basename(filepath)}") 
    print("=" * 70)
    text = read_text_file(filepath)

    word_count = count_words(text)
    words_ucount = count_unique_words(text)
    lines_count = count_lines(text)
    ttr_count = calculate_ttr(text)

    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤: {word_count}")
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {words_ucount}")
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫: {lines_count}")
    print(f"–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Ç–∏–ø-—Ç–æ–∫–µ–Ω (TTR): {round(ttr_count, 3)}")

    top_words = get_most_common_words(text)
    print(f"\n–ù–∞–∏–±–æ–ª–µ–µ —É–ø–æ—Ç—Ä–µ–±–ª—è–µ–º—ã–µ —Å–ª–æ–≤–∞: {top_words}")

def analyze_corpus(corpus_folder):
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤—Å–µ —Ç–µ–∫—Å—Ç—ã –≤ –ø–∞–ø–∫–µ, —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ –≤—ã–≤–æ–¥–∏—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É.

    Args:
        corpus_folder (str): –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å —Ç–µ–∫—Å—Ç–∞–º–∏ 
    """
    print("=" * 70)
    print("üìä –ê–Ω–∞–ª–∏–∑ –∫–æ—Ä–ø—É—Å–∞ —Ç–µ–∫—Å—Ç–æ–≤")
    print("=" * 70)

    # 1. –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤ –∏–∑ –ø–∞–ø–∫–∏
    files = get_files_in_folder(corpus_folder, '.txt')
    
    # –°–æ–∑–¥–∞—ë–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞
    data = []
    all_text = ""

    # 2. –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ –∫–∞–∂–¥–æ–º—É —Ñ–∞–π–ª—É –∏–∑ —Å–ø–∏—Å–∫–∞
    for filename in files:
        # –°—Ç—Ä–æ–∏–º –ø–æ–ª–Ω—ã–π –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É
        filepath = os.path.join(corpus_folder, filename)
        # –ß–∏—Ç–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ —Ñ–∞–π–ª–∞
        text = read_text_file(filepath)
        all_text += text + " "     # –î–æ–±–∞–≤–ª—è–µ–º –∫ –æ–±—â–µ–º—É —Ç–µ–∫—Å—Ç—É —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–º
        # –°—á–∏—Ç–∞–µ–º —Å–∫–æ–ª—å–∫–æ —Å–ª–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ
        word_count = count_words(text)
        words_ucount = count_unique_words(text)
        lines_count = count_lines(text)
        ttr_count = calculate_ttr(text)
        data.append([filename, word_count, words_ucount, lines_count, ttr_count])
    
    # 3. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ CSV —Ñ–∞–π–ª
    results_folder = 'results'
    # –°–æ–∑–¥–∞—ë–º –ø–∞–ø–∫—É results, –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
    os.makedirs(results_folder, exist_ok=True)
    
    csv_file_path = os.path.join(results_folder, 'statistics.csv')
    write_csv_file(csv_file_path, 
                   ['filename', 'word_count', 'words_ucount', 'lines_count', 'ttr_count'], 
                   data)

    print(f"\n‚úì –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(data)}")
    print(f"‚úì –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {csv_file_path}")

    # 4. –ó–∞–≥—Ä—É–∂–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–∑ CSV —Ñ–∞–π–ª–∞
    stats = read_csv_file(csv_file_path)

    print("\nüìñ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ñ–∞–π–ª–∞–º:\n")

    # 5. –í—ã–≤–æ–¥–∏–º –∏–º—è –∫–∞–∂–¥–æ–≥–æ —Ñ–∞–π–ª–∞ –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤
    total_words = 0  # –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Å—É–º–º—ã –≤—Å–µ—Ö —Å–ª–æ–≤
    for i, row in enumerate(stats, start=1):
        # –ü–æ–ª—É—á–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ (—Å—Ç—Ä–æ–∫–∞ ‚Üí —á–∏—Å–ª–æ)
        filename = row['filename']
        word_count = int(row['word_count'])
        words_ucount = int(row['words_ucount'])
        lines_count = int(row['lines_count'])
        ttr_count = round(float(row['ttr_count']), 3)
        print(f"{i}. {filename}: {word_count}, {words_ucount}, {lines_count}, {ttr_count}")
        total_words += word_count

    # 6. –í—ã–≤–æ–¥–∏–º –æ–±—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É: –≤—Å–µ–≥–æ —Å–ª–æ–≤ –∏ —Å—Ä–µ–¥–Ω–µ–µ –Ω–∞ —Ñ–∞–π–ª
    print("\nüìà –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(f"   –í—Å–µ–≥–æ —Ç–µ–∫—Å—Ç–æ–≤ –≤ –∫–æ—Ä–ø—É—Å–µ: {len(files)}")
    print(f"   –í—Å–µ–≥–æ —Å–ª–æ–≤ –≤ –∫–æ—Ä–ø—É—Å–µ: {total_words}")
    # –ß—Ç–æ–±—ã –Ω–∞–π—Ç–∏ —Å—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ ‚Äî –¥–µ–ª–∏–º –Ω–∞ —á–∏—Å–ª–æ —Ñ–∞–π–ª–æ–≤
    if len(stats) > 0:
        average = total_words // len(stats)
    else:
        average = 0
    print(f"   –°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤: {average}")
    top_words = get_most_common_words(all_text)
    print(f"\n–ù–∞–∏–±–æ–ª–µ–µ —É–ø–æ—Ç—Ä–µ–±–ª—è–µ–º—ã–µ —Å–ª–æ–≤–∞ –≤–æ –≤—Å—ë–º –∫–æ—Ä–ø—É—Å–µ: {top_words}")
    
    return csv_file_path  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Ç—å –∫ CSV —Ñ–∞–π–ª—É

def load_csv_data(filepath):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ CSV —Ñ–∞–π–ª–∞.
    
    Args:
        filepath (str): –ü—É—Ç—å –∫ CSV —Ñ–∞–π–ª—É
        
    Returns:
        list: –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –¥–∞–Ω–Ω—ã–º–∏
    """
    data = []
    try:
        with open(filepath, 'r', encoding='utf-8') as file:
            reader = csv.DictReader(file)
            for row in reader:
                data.append(row)
        return data
    except FileNotFoundError:
        print(f"‚úó –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {filepath}")
        return []
    except Exception as e:
        print(f"‚úó –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {filepath}: {e}")
        return []

def prepare_results_for_report(csv_filepath):
    """
    –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ statistics.csv –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è generate_report.
    
    Args:
        csv_filepath (str): –ü—É—Ç—å –∫ statistics.csv
        
    Returns:
        list: –î–∞–Ω–Ω—ã–µ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
    """
    raw_data = load_csv_data(csv_filepath)
    if not raw_data:
        return []
    
    results = []
    for row in raw_data:
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ CSV –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è –æ—Ç—á–µ—Ç–∞
        result = {
            'filename': row.get('filename', ''),
            'word_count': int(row.get('word_count', 0)),
            'words_ucount': int(row.get('words_ucount', 0)),  # –û—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
            'ttr_count': float(row.get('ttr_count', 0.0)),    # –û—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
            'lines_count': int(row.get('lines_count', 0))     # –î–æ–±–∞–≤–ª—è–µ–º –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        }
        results.append(result)
    
    return results

def generate_report(results, metadata):
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á—ë—Ç —Å –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ–º –¥–∞–Ω–Ω—ã—Ö –∞–Ω–∞–ª–∏–∑–∞ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö.

    Args:
        results (list): –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞
                       (–¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∫–ª—é—á–∏: filename, word_count, words_ucount, ttr_count)
        metadata (list): –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –æ —Ç–µ–∫—Å—Ç–∞—Ö

    Returns:
        str: –¢–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á—ë—Ç
    """
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –¥–∞–Ω–Ω—ã–µ
    if not results:
        return "–û—à–∏–±–∫–∞: –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á–µ—Ç–∞."
    
    # –°–æ–∑–¥–∞—ë–º —Å–ª–æ–≤–∞—Ä—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
    metadata_map = {}
    if metadata:
        try:
            for item in metadata:
                if 'filename' in item:
                    metadata_map[item['filename']] = item
        except Exception as e:
            print(f"–í–Ω–∏–º–∞–Ω–∏–µ: –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {e}")

    # –ù–∞—á–∏–Ω–∞–µ–º —Å–æ—Å—Ç–∞–≤–ª—è—Ç—å –æ—Ç—á—ë—Ç
    report_lines = []
    report_lines.append("=" * 70)
    report_lines.append("üìä –û–¢–ß–Å–¢ –ü–û –ê–ù–ê–õ–ò–ó–£ –ö–û–†–ü–£–°–ê –¢–ï–ö–°–¢–û–í")
    report_lines.append("=" * 70)

    # ========== –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê ==========
    report_lines.append("\nüìà –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
    report_lines.append("-" * 70)

    # –°—á–∏—Ç–∞–µ–º –æ–±—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏
    total_files = len(results)
    total_words = sum(r.get('word_count', 0) for r in results)
    total_unique = sum(r.get('words_ucount', 0) for r in results)  # –ò—Å–ø–æ–ª—å–∑—É–µ–º words_ucount
    avg_ttr = sum(r.get('ttr_count', 0) for r in results) / len(results) if results else 0  # –ò—Å–ø–æ–ª—å–∑—É–µ–º ttr_count

    report_lines.append(f"  –í—Å–µ–≥–æ —Ç–µ–∫—Å—Ç–æ–≤ –≤ –∫–æ—Ä–ø—É—Å–µ: {total_files}")
    report_lines.append(f"  –í—Å–µ–≥–æ —Å–ª–æ–≤: {total_words}")
    report_lines.append(f"  –í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {total_unique}")
    report_lines.append(f"  –°—Ä–µ–¥–Ω–∏–π Type-Token Ratio (TTR): {round(avg_ttr, 3)}")

    # ========== –î–ï–¢–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê ==========
    report_lines.append("\nüìÑ –î–ï–¢–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –§–ê–ô–õ–ê–ú:")
    report_lines.append("-" * 70)

    for i, result in enumerate(results, start=1):
        filename = result.get('filename', f'–§–∞–π–ª_{i}')
        meta = metadata_map.get(filename, {})

        report_lines.append(f"\n{i}. {filename}")
        report_lines.append(f"   –ù–∞–∑–≤–∞–Ω–∏–µ: {meta.get('title', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}")
        report_lines.append(f"   –ê–≤—Ç–æ—Ä: {meta.get('author', '–ù–µ–∏–∑–≤–µ—Å—Ç–µ–Ω')}")
        report_lines.append(f"   –ì–æ–¥: {meta.get('year', 'N/A')}")
        report_lines.append(f"   –°–ª–æ–≤: {result.get('word_count', 'N/A')}")
        report_lines.append(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {result.get('words_ucount', 'N/A')}")  # –ò—Å–ø–æ–ª—å–∑—É–µ–º words_ucount
        report_lines.append(f"   TTR: {result.get('ttr_count', 0):.3f}")  # –ò—Å–ø–æ–ª—å–∑—É–µ–º ttr_count –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º

    # ========== –í–´–í–û–î–´ ==========
    report_lines.append("\n" + "=" * 70)
    report_lines.append("üìå –í–´–í–û–î–´ –ò –ò–ù–¢–ï–†–ü–†–ï–¢–ê–¶–ò–Ø:")
    report_lines.append("=" * 70)

    if results:
        # –ù–∞—Ö–æ–¥–∏–º —Ç–µ–∫—Å—Ç —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º TTR (–ª–µ–∫—Å–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ)
        max_ttr_result = max(results, key=lambda x: x.get('ttr_count', 0))  # –ò—Å–ø–æ–ª—å–∑—É–µ–º ttr_count
        min_ttr_result = min(results, key=lambda x: x.get('ttr_count', 0))  # –ò—Å–ø–æ–ª—å–∑—É–µ–º ttr_count

        report_lines.append(f"\n1. –õ–µ–∫—Å–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ:")
        report_lines.append(
            f"   ‚Ä¢ –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ: {max_ttr_result.get('filename', 'N/A')} "
            f"(TTR = {max_ttr_result.get('ttr_count', 0):.3f})"  # –ò—Å–ø–æ–ª—å–∑—É–µ–º ttr_count
        )
        report_lines.append(
            f"   ‚Ä¢ –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ: {min_ttr_result.get('filename', 'N/A')} "
            f"(TTR = {min_ttr_result.get('ttr_count', 0):.3f})"  # –ò—Å–ø–æ–ª—å–∑—É–µ–º ttr_count
        )

    report_lines.append(f"\n2. –û–±—â–∏–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è:")
    report_lines.append(
        f"   ‚Ä¢ –°—Ä–µ–¥–Ω–∏–π TTR –≤—Å–µ–≥–æ –∫–æ—Ä–ø—É—Å–∞ —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç {round(avg_ttr, 3)}, "
        f"—á—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ {'–≤—ã—Å–æ–∫–æ–µ' if avg_ttr > 0.6 else '—Å—Ä–µ–¥–Ω–µ–µ' if avg_ttr > 0.4 else '–Ω–∏–∑–∫–æ–µ'} "
        f"–ª–µ–∫—Å–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ —Ç–µ–∫—Å—Ç–æ–≤."
    )
    report_lines.append(
        f"   ‚Ä¢ –í—Å–µ–≥–æ –≤ –∫–æ—Ä–ø—É—Å–µ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {total_words} —Å–ª–æ–≤ "
        f"–∏ –Ω–∞–π–¥–µ–Ω–æ {total_unique} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤."
    )

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å—ë –≤ –æ–¥–∏–Ω —Ç–µ–∫—Å—Ç
    return "\n".join(report_lines)

def get_report():
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç—á–µ—Ç–∞.
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ñ–∞–π–ª–æ–≤ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ—Ç—á–µ—Ç –≤ results/report.txt
    """
    
    # 1. –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ—Ç—á–µ—Ç–∞
    results_data = prepare_results_for_report('results/statistics.csv')
    
    if not results_data:
        print("‚úó –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏")
        return
    
    # 2. –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    metadata_path = 'data/metadata.csv'
    metadata_data = load_csv_data(metadata_path)
    
    if not metadata_data:
        print("‚ö† –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã, –æ—Ç—á–µ—Ç –±—É–¥–µ—Ç –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏")
    
    # 3. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç
    report_text = generate_report(results_data, metadata_data)
    
    # 4. –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç –≤ —Ñ–∞–π–ª
    report_path = 'results/report.txt'
    try:
        # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –ø–∞–ø–∫–∞ results —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        os.makedirs('results', exist_ok=True)
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_text)   
        print(f"‚úÖ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {report_path}")
        
    except Exception as e:
        print(f"‚úó –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –æ—Ç—á–µ—Ç–∞: {e}")

if __name__ == '__main__':
    # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
    
    # 1. –ê–Ω–∞–ª–∏–∑ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    # filepath = "corpus/poem_01.txt"  
    # analyze_text_file(filepath)
    # print("\n" + "=" * 70 + "\n")
    
    # 2. –ê–Ω–∞–ª–∏–∑ –≤—Å–µ–≥–æ –∫–æ—Ä–ø—É—Å–∞
    stats_file = analyze_corpus('corpus')
    
    print("\n" + "=" * 70 + "\n")
    
    # 3. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞
    get_report()